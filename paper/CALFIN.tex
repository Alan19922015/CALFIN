%Tim Dixon
%%Main takeaway - we provide calving front locations, and have automated it
%Some may be more interested in the data, or the method, so make sure we cover both

\documentclass[tc, manuscript]{copernicus}

\usepackage{wrapfig}
\usepackage{multirow}
\begin{document}

\title{Calving Front Machine (CALFIN): Automated Calving Front Mask Dataset and Deep Learning Methodology for East/West Greenland, 1972-2019}

\Author[1]{Daniel}{Cheng}
\Author[1]{Yara}{Mohajerani}
\Author[2]{Michael}{Wood}
\Author[1]{Isabella}{Velicogna}
\Author[2]{Eric}{Larour}
\Author[1]{Wayne}{Hayes}

\affil[1]{University of California at Irvine, Irvine CA, USA}
\affil[2]{Jet Propulsion Laboratory, California Institute of Technology, Pasadena CA, USA}

\runningtitle{TEXT}

\runningauthor{TEXT}

\correspondence{Daniel Cheng (dlcheng@uci.edu)}

\received{}
\pubdiscuss{} %% only important for two-stage journals
\revised{}
\accepted{}
\published{}

\maketitle

\begin{abstract}
We present Calving Front Machine (CALFIN), a dataset of calving front masks for 66 glaciers along East/West Greenland. The dataset is generated from Landsat imagery from 1972 to 2019. This dataset is unique in scope, and provides new constraints on glacial evolution over the time period. The current iteration offers a new opportunity to explore previous trends and validate existing models moving forward.

The dataset contains data that is automatically generated. We present our automated methodology, which utilizes deep learning in the form of the CALFIN Neural Network. This approach builds on existing work \citep{mohajerani2019} \citep{zhang2019} \citep{baumhoer2019}. The network itself is derived from U-Net/DeeplabV3+ Xception architectures \citep{ronneberger2015} \citep{chen2018}. Additional post-processing techniques allow our method to achieve accurate and useful segmentation of raw images into Shapefile outputs. This methodology is uniquely robust to clouds, illumination differences, ice mélange, and Landsat-7 Scan Line Corrector errors.

We perform an analysis of CALFIN results, which approach human levels of accuracy in comparison to manually delineated fronts. Additionally, we perform a model inter-comparison to evaluate CALFIN's performance against existing methodologies. We showcase CALFIN's ability to generalize to images it has not trained on. We achieve a mean error of 80.10 meters from the true front, on a set of 162 testing images. %TODO/Save for future paper? We show the applicability of CALFIN's methodology to SAR data, Antarctic glaciers, as well as non-marine terminating glaciers.

At this stage, we seek feedback from the community and welcome any critiques or questions regarding the dataset and/or our methods. This work was conducted as a collaboration between NASA’s Jet Propulsion Laboratory and the University of California, Irvine.
\citep{ronneberger2015}
\citep{iglovikov2018}
\citep{chollet2016}
\citep{chen2017}
\citep{chen2018}
\citep{bjork2015}
\citep{xie2015}
\citep{garcia2012}

\end{abstract}

\introduction
%Introduction
The evolution of Greenland's tidewater glaciers is an important constraint on the evolution of the Greenland Ice Sheet as a whole. Likewise, changes in the Greenland are important in tracking and predicting changes in the climate overall. Constraining Greenland's glacial evolution is thus an important part of improving our understanding of Earth's climate.

%Comment out subsection headers?
\subsection{Motivation}
One such constraint on glacial evolution is the position of glacial calving fronts over time. However, delineating the calving fronts of marine-terminating glaciers is typically a time-intensive manual process. As a result, many glaciers receive less labeling than is ideal, or none at all. Additionally, existing glaciers that do receive annual labeling do not capture intra-annual variability. 

The need for an improved methodology is apparent. An automated methodology allows for the solution to previously stated issues. However, the automation of glacial calving front delineation is non-trivial. Confounding issues - such as cloud cover, ice mélange, shadowing,  - present an impediment to naive techniques such as edge detection \citep{paravolidakis2016} and texture analysis \citep{malik2001}. Ultimately, emerging techniques in computer science provide a promising approach to this problem. More specifically, machine learning and deep neural networks provide a robust, scalable, and accurate method to automatically delineate tidewater glacial calving fronts.

\subsection{Methodology}
In this study, we evaluate automated methods of delineating glacial termini. The glaciers studied are located along East/West Greenland. They include Helheim, Kangerlussuaq, Kong Oscar, Hayes, Rink Isbrae, Upernavik, Jakobshavn, Kangiata Nunaata, and 58 other nearby glaciers. The intra-annual time series studied spans from 1972 to June 2019. Our source data is the Landsat Near Infrared (NIR) band.

We determine that convolutional neural networks present the most promising automated methodology. We determine that the UNet-inspired DeeplabV3+ architecture, with Xception backbone, is the most promising neural network architecture. We release a modified version of the DeeplabV3+ network, along with the final weights we use for dataset production. We release a set of data products, Calving Front Machine (CALFIN), which consists of both calving front masks, as well as Shapefile polylines for the 66 glacial basins along East/West Greenland.

\subsection{Existing Work}
Existing efforts have been made by ESA's Climate Change Initiative to provide calving front locations for 26 Greenlandic basins, from 1990-2016\citep{enveo2017}. Additionally, the Programme for Monitoring of the Greenland Ice Sheet provides calving fronts for 47 glaciers, but is limited to annual measurements from 1990-2018\citep{andersen2019}. Moroever, there are growing efforts to provide a unified database of manually delineated calving fronts. Nonetheless, the constant cost of adding of new data - as well as the current gaps in shared data - implies a consistent need for new data assimilation efforts.

We evaluate existing work in the field, performed by Mohajerani, Zhang, and Baumhoer. These studies offer the potential to cross-validate the viability of our own methods.

\subsection{Takeaways}
Our main takeaway is the viability of generalized neural networks for automated calving front detection. Specifically, a well trained network can approach human levels of accuracy in picking arbitrary glacial calving fronts - without retraining. This demonstration of learning also lends credence to this methodology as a viable approach to other data assimilation tasks.

Our key goal is to provide high spatial accuracy, dense temporal resolution, and long time series calving front masks for the cryospheric/climate modeling community. We look forward to fulfilling our part in this collaborative goal, as we continue to improve our method and release additional data moving forward.

\section{Data - Source and Preprocessing}
This study focuses on the automated data assimilation of remote sensing data. In this section, we describe the following: we evaluate data sources with high spatio-temporal resolution; after selecting Landsat as our data source, we describe the preprocessing methods required for input into our neural network processing step.

\subsection{Data - Source}

\begin{wrapfigure}{r}{9.0cm}
\noindent
\begin{minipage}{9.0cm}
    \caption{A comparison of the data sources available for use. Long time series, high resolution, high repeat cycle attributes are desired, in that order. Italicized entries represent optimal sources within each attribute. }
    \label{tab:table-data_source}
    \resizebox{9.0cm}{!}{
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    \multicolumn{5}{|c|}{\textbf{Potential Data Sources}} \\ \hline
    \textbf{Name} & \textbf{Resolution(s)} & \textbf{Time Series} & \textbf{Repeat Cycle} & \textbf{Sensor} \\ \hline
    Landsat & 30m, 60m & \textit{1972-present} & 16 day & Optical \\ \hline
    Terra (MODIS) & 250m, 500m, 1000m & 1999-present & \textit{1, 8, 16 day} & Optical \\ \hline
    Sentinel & 10m, 20m, 60m & 2014-present & 10, 12 day & SAR \\ \hline
    TerraSAR-X & \textit{1m, 3m, 6m} & 2007-present & 3-11 day & SAR \\ \hline
    
    \end{tabular}
    }
\end{minipage}

\begin{minipage}{9.0cm}
    \includegraphics[width=9.0cm]{all_Landsat_bands_annotated.png}
    \caption{Landsat band availability overlaid a graph of atmospheric transmission percentages for various wavelengths. The Near-Infrared The NIR band (highlighted in pink) benefits from both long time series availability, as well as high atmospheric transmittance.}
\end{minipage}
\end{wrapfigure}

We evaluate several potential dataset sources that provide raw data from which we assimilate calving front positions. These include Terra/MODIS, TerraSAR-X, Landsat, and Sentinel. We compare the potential data sources' attributes in Figure 1. We select Landsat for its long time-series availability and reasonable spatial distribution/resolution.

We constrain our input to be based on subsets of a single band. This is due to the computational constraints of processing multiband inputs, and differences between available bands across Landsats 1-8. Another consideration is the diminishing returns of adding additional bands. Since pixel information is correlated across different bands, and since a single band image already shows any discernible calving fronts, additional band information was deemed unnecessary.

%Insert landsat band overlap table
%https://landsat.gsfc.nasa.gov/landsat-9/landsat-9-spectral-bands/
We pick the Near Infrared (NIR) (.77-1.1$\mu m$) wavelength band. This is based on evaluating  Landsat 1 - 8 band commonality, and choosing the band with the longest time series availability (See Figure 2). We then pick the band with the highest atmospheric transmittance, which allows us to "look past" light cloud cover. We initially hypothesized that using a common band is beneficial, as different spectral bands may exhibit features differently, and thus negatively impact detection accuracy. However, in practice, the band selection and data source are inconsequential as long as calving fronts are visible (see 7.3 Discussion - Inter-model Comparison).

%%FIX THESE FIGURES - UPDATE TEMPORAL RESOLUTION

\begin{wrapfigure}{r}{12.1cm}
    \noindent
    \begin{minipage}{4.8cm}
        \includegraphics[width=4.8cm]{temporal.png}
        \caption{Number of images across the years for 8 selected glaciers. We evaluate glaciers across a longer time series than any existing studies.}
    \end{minipage}
    \hspace{0.15cm}
    \begin{minipage}{7.15cm}
        \centering
        \includegraphics[width=7.15cm]{spatial_66.png}
        \caption{Spatial distribution of 66 glaciers along West Greenland. We study glaciers with reasonable velocities and wide spatial distribution. Velocity map taken from Nagler \citep{nagler2015}.}
    \end{minipage}
\end{wrapfigure}

\subsubsection{Spatio-Temporal Coverage}
With Landsat as our data source, we are able to determine CAFLIN's spatial extent and time series coverage. We provide calving fronts for 66 basins along East/West Greenland, spanning the period from 1972 to 2019. We provide intra-annual samplings of each basin. The exact number of samples varies widely for each basin. Each sample may be missing from the dataset due to heavy clouds, coverage gaps, or a lack of confidence in the processed output (i.e., CALFIN can prune samples where it cannot clearly determine the calving front). 

We select 8 primary basins for which we desire the densest temporal coverage. These basins are primarily selected for study potential/high velocities. Basins are secondarily selected to obtain a uniform spatial sampling of coastline.

We provide the primary temporal coverage chart (Figure 3) and the full spatial coverage map (Figure 4) on this page. For the full temporal coverage chart showing all 66 basins, see the attached Supplement.

\subsection{Data - Preprocessing}
Throughout our study, we developed a pipeline that automates much of the data preprocessing that is required for this task. This pipeline involves the following steps: Data source collection, GeoTIFF/domain subsetting, Low quality/clouded image pruning, Subset standardization, Detail Enhancement.
Once these steps are complete, we proceed with processing in the neural network processing step. See Figure 5 for a visual outline of these steps.


\begin{figure}[t]
\includegraphics[width=14cm]{pipeline-preprocess.png}
\centering
\caption{A visual outline of preprocessing steps. Each is explained in more detail in the following subsections.}
\end{figure}

\subsubsection{Data - Preprocessing - Collection}
As a first step, we collect the input raster images that for which we want to evaluate. We begin by selecting all rasters centered around  one of 8 primary glacial basins along East/West Greenland. These basins include Kong Oscar, Hayes, Rink Isbrae, Upernavik, Jakobshavn, Kangiata Nunaata, Helheim, and Kangerlussuaq. These basins are selected both for their spatial distribution as well as their high flow velocities. These characteristics are important for ensuring wide spatial coverage and having dynamic calving fronts. For each domain, we filter for all Level 1 TP rasters. We add selected L1GS/L1GT products, which we manually georeference, to fill in Landsat 1-2 time series gaps (1972-1985). We further filter for rasters with less than 20\% cloud cover across the entire area. At this stage, we accumulate a total of 4776 Landsat rasters, from which we subset individual glacial domains. 

\subsubsection{Data - Preprocessing - Subsetting}
To begin subsetting, we define the domain of a glacial basin, which should fully encloses the calving front being extracted. We encode the domain as a rectangular Shapefile polygon. Next, we automatically subset the raster for every time step which we want to evaluate for the domain. We repeat this process for each basin to be studied. While this requires manual input, it only needs to be done once, as the domains will be reused across multiple subsetting operations. For rasters and domains in West Greendland, we reproject to WGS 84 / UTM zone 21N (EPSG:32621). For East Greenland, we use WGS 84 / UTM zone 24N (EPSG:32624).

\subsubsection{Data - Preprocessing - Pruning}
After subsetting, we automatically prune images that are ill-suited for further processing. First, we determine the amount of NODATA pixels present in the subset, and remove it from evaluation if it exceeds a predetermined threshold (>30\%). This allows us to process images with Landsat 7 scan-line errors and domains along raster boundaries, while still filtering out largely out-of-bounds subsets. This prevents them from being unnecessarily and inaccurately processed. Also, if an accompanying Quality Assurance band is present, the same filtering procedure is performed to eliminate images with high cloud cover, for the same reasons. We accumulate 20,029 GeoTIFF subsets, spread across 66 glacial domains.

\subsubsection{Data - Preprocessing - Standardization}
Now we resize the image to a standardized, square resolution. We convert the subsets from their native GeoTIFF formats to a standardized 256x256 PNG image. This step can affect the accuracy of the delineation, which we address by defining domains as close to the square aspect ratio whenever possible.

\subsubsection{Data - Preprocessing - Detail Enhancement}
Lastly, we perform image transformations to enhance detail and provide the processing step with more information. We found through empirical testing that this step can specifically benefit raw images with heavy shadowing. This benefit emerges since these images contain details which are otherwise undetectable by the filters used in the processing step. We utilize two non-linear transformations, the Pseudo-HDR Toning and Shadows and Highlights enhancements. These enhancements are performed with default settings in Adobe Photoshop, but we aim to write an open-source equivalent in Python in the future. We concatenate the results of the two transformations with the raw image. This creates 3-channel RGB input images that contains information to be be utilized by the next step. 

At this point, the images are ready for processing into calving front masks.


\section{Methodology - Processing}
We perform the core processing step by passing images through the Calving Front Machine Neural Network (CALFIN-NN). As a convolutional neural network that inherits from the U-Net family of architectures, CALFIN-NN is capable of learning abstract image features, and outputting confidence masks of where it detects coastlines  and calving fronts. Importantly, it generalizes to new data automatically. This key capability is the foundation of most other automated methods in this field, including those by Zhang \citep{zhang2019}, Mohajerani \citep{mohajerani2019}, and Baumhoer \citep{baumhoer2019}.

Our network builds upon this work, and utilizes a modification of Google's DeepLabV3+ Xception \citep{chen2018} image segmentation network. With our customized architecture and training regimen, CALFIN-NN is capable of handling inputs with different scales/resolutions, heavy shadowing, ice mélange, light cloud cover, and Landsat 7 scan-line errors. Together, these novel innovations bring the state-of-the-art in image segmentation to the cryosphere community.

Our implementation of this processing step begins by splitting up the 256x256x3 pre-processed images into 9 overlapping 224x224x3 image patches. We exploit the overlapping windows to allow for both a smaller network size, as well as  to create a confidence estimate in our predictions over multiple processing passes per pixel.

After processing each input image, the  outputting a coastline/calving front mask is ready to be post-processed.

The following section goes into additional detail regarding aspects of our neural network model. Further details - including the iterative process of testing, evaluation methods, and our final training regimen - are described in Discussion. 

\subsection{Model Architecture \& Modifications}

\begin{figure}[t]
\includegraphics[width=18cm]{arch_final.png}
\centering
\caption{CALFIN-NN model architecture. The first half, the encoder, consists of multi-scale feature extraction blocks (Orange Xception modules, Atrous Spatial Pyramidal Pooling block). Note blocks in the middle flow section have been omitted for clarity (MB x7 represents 7 more sequential middle blocks). The second half, the decoder, up-scales the features and localizes the calving front at higher resolutions. CALFIN-NN belongs to the UNet family of architectures, and inherits directly from the DeepLabV3+ Xception network.}
\end{figure}

CALFIN-NN takes in 3-channel pre-processed images, as detailed in section 2.2.5.
CALFIN-NN outputs 2-channel images containing the coastline/calving front edge in the first (Red) channel, and the land ice/ocean mask in the second (Green) channel.
The model architecture consists of a modified DeepLabV3+ Xception image segmentation network, as shown in Figure 5. 
The first half of the network - the encoder - uses the Xception-65 network \citep{chollet2016} as our feature extractor. These features encode information in the image using higher-order image kernels. In other words, it assembles basic features (such as edges, corners) into more abstract features (such as glacier/land textures). DeepLabV3+ adds in a Atrous Spatial Pyramidal Pooling (ASPP) module that further enhances multi-scale feature detection, as each separate block in the ASPP module uses kernels dilated to a different scale. 
The second half of the network - the decoder - takes the output of the encoder and the ASPP module, up-samples the features and predicts the final output before performing one last up-sampling to the original input/output size. Note that no additional convolution kernels are used after the final up-sampling, and the usage of 4x up-sampling instead of 2x up-sampling. See the DeepLabV3+ paper by Chen et al. for insight into the reasoning for these design decisions \citep{chen2018}.

We make several modifications to the basic DeepLabV3+ Xception model architecture. Since DeepLabV3+ Xception is optimized for segmenting and classifying objects, it requires changes to accurately recognize thin/line features like calving fronts. To this end, we add additional ASPP blocks with different dilation scales (6, 12, 18 in the original, compared to 1, 2, 3, 4, 5 in CALFIN-NN). Furthermore, we modify the deepest pooling layer to only downsample by 2 instead of 14. These changes account for differences in input size when compared to the original model (224 vs 512) and enable better multi-scale feature localization. We switch the encoder module from the Xception 71 layer model to the Xception 65 layer model and remove the header module of DeepLabV3+, as both reduced the resolution of the image too much for accurate edge localization. We also modified tail of the network to output 2 channels for mask and edge simultaneously. These changes both reduce the weight size from ~40m parameters to ~29m parameters (~400MB on disk) as well as increase the overall accuracy of the network.

Ultimately, these modifications and the neural network architecture itself are the key innovations that allows for automatic processing of calving front images.

\section{Post-Processing}
At this stage, the 2-channel pixel mask output of CALFIN-NN must be post-processed in order to create a useful data product. However, this task is non-trivial, and is subject to several unique constraints. We describe the various problems we encounter, and offer solutions for each.
\begin{itemize}
\item The pixel mask output by CALFIN-NN is often imperfect. We develop a unique algorithm to infer the correct coastline boundary.
\item The calving front must still be isolated from the coastline that is output by CALFIN-NN. We utilize fjord boundary masks and image analysis techniques to accomplish this.
\item The isolated calving fronts must then be isolated and refined, as the fixed input/output resolution of CALFIN-NN results in error associated with subset rescaling. We address this by performing a pseudo box-regression, and re-processing zoomed-in subsets that are centered on each isolated calving front.
\item Each refined calving front must still be transformed into a georeferenced Shapefile for further use. This is comparatively easy to solve using GDAL and the original GeoTIFF subsets, but still must be performed.
\end{itemize}
The following subsections cover our solutions to each of the problems above. See Figure 7 for a visual outline of these steps.

\begin{figure}[t]
\includegraphics[width=14cm]{pipeline-postprocess.png}
\centering
\caption{A visual outline of postprocessing steps. Each is explained in more detail in the following subsections.}
\end{figure}

\subsection{Post-Processing - Pixel Mask to Coastal Polyline}
The first problem is that the raw pixel mask is often imperfect. Currently, CALFIN-NN cannot always completely determine the calving front in an arbitrary image. As a consequence, traditional vectorization methods using OpenCV or GDAL will not work with the imperfect, non-closed contours seen in  CALFIN-NN's output. However, the nature of its inaccuracies are predictable. Thus we exploit the characteristics of CALFIN-NN's output to compensate for its specific inaccuracies. $show examples here!$

To address this first constraint, we develop an algorithm that finds the polyline representing the coastline in the pixel mask. This algorithm shares similarities to (but is independent from) Michał Żołnieruk's point cloud to line approximation method \citep{żołnieruk2016}. Our algorithm is implemented by converting each pixel in the mask to nodes in a graph, and finding the single longest path in the graph's minimum spanning tree. This path corresponds to the the coastline edge. Note that this algorithm is not guaranteed to find the correct calving front, as it is still limited by the correctness of CALFIN-NN's outputs.

A detailed explanation of the algorithm follows below.

%Include figure of pipeline here or condense?

\begin{enumerate}
    \item First, calculate the euclidean distances between each node and every other node, creating distance matrix D.
    \item Then for each point, find the k-nearest neighbors graph using distance matrix D. This ensures a connected path with continuous lines, while still allowing for large gaps in the pixel mask to be crossed. K is empirically chosen to be the maximum of some minimum number of nodes (we choose 4) and some percentage of the total number of nodes (we choose 20\%). The choice of K is influences outlier detection in the following step. 
    \item To address outliers and spurious detections, we remove any nodes that mean distance to its neighbors is itself an outlier. To compute the outliers of mean neighbor distance, we use the robust Modified Z-score method with Median Absolute Deviation \citep{garcia2012}. We eliminate any node whose mean neighbor distance exceeds a z-score of 10. We determine this threshold through empirical testing. 
    \item Next, we use the k-nearest neighbors graph to construct a undirected weighted minimum spanning tree (MST).
    \item Lastly, we find the longest path in the MST. This can be done by performing 2 depth-first searches (DFS), with each DFS finding one endpoint along the longest overall path. This longest overall in the MST consistently corresponds with the calving front detected by CALFIN-NN.
\end{enumerate}

\subsection{Post-Processing - Coastline to Calving Front}
The second problem is to isolate the calving front from the coastline polyline output by the previous post-processing step. This problem occurs because CALFIN-NN is trained to detect ice/ocean boundaries, but not yet how to isolate the fronts themselves. Therefore, work must be done to remove edges detected along the fjord boundaries.

We address this second constraint by creating 66 fjord boundary masks, one for each domain. We then use these masks to extract the front from the full coastline edge mask that is output by the neural network. Still, due to coastline variations, the static fjord boundary mask does not always isolate the calving front. Thus we develop an algorithm to isolate the calving front from the coastline. We exploit the fact that pixel in the detected coastline are close to the fjord boundaries, while pixel in the calving front are usually far from both. We calculate the distance from each point in the coastline to the nearest fjord boundary pixel, then select the contiguous pixels which are the farthest from the rest. This results in calving front masks that terminate at the fjord boundaries, and are ready for further post-processing. In our future work, this step can be integrated directly into the neural network. This can be done by adding a third output CALFIN-NN output channel that indicates where the calving fronts are. Additionally, accuracy can be improved with more accurate fjord boundaries.


\subsection{Post-Processing - Calving Front Reprocessing}
The third problem arises from the fact that our image subsets encompass the entire range of potential calving fronts over the period 1972-2019. This results in lower accuracy when extracting the calving front, due to the lower spatial resolution of the input imagery when it is resized during preprocessing. Additionally, there may be multiple fronts depending the specific domain and time frame. %Show example

We address this third constraint by exploiting CALFIN-NN's multi-scale capabilities, and reprocessing a subset of the image subset. We perform this operation doing a second processing pass, where the first one detects the fronts, and the second one "zooms in" on each detected front. This implementation mimics a bounding box regression and localization routine, commonly found in other neural networks. Furthermore, each individual front may or may not be confidently detected. We exploit the nature of CALFIN-NN's output as a confidence measure, so that generated fronts can be filtered out based on confidence thresholds, and overall accuracy can be increased. In the future, this step can be integrated directly into the network as in Mask Region-based CNNs \citep{he2017}.

\subsection{Post-Processing - Calving Front to Shapefile}
The last problem is to correctly transform the vectorized polyline to a geo-referenced Shapefile. While this problem is more easily solvable, it must still be performed in order to generate the desired data product. 

We perform a final interpolation of the vertices to smooth We address this last constraint by using GDAL and the GeoTIFF subsets from the preprocessing step to re-scale the polyline, georeference it, and export a Shapefile data product.


\section{Quality Assessment and Error Analysis}
We select a validation set to evaluate the ability of CALFIN to generalize to new data. We have two primary methods of evaluating the predicted output of any calving front against the ground truth labeling. We calculate classification accuracy with the Jaccard Index, and estimate the error by calculating the Distance from the True Front. We also calculate detection accuracy and an associated confusion matrix.

\subsection{Validation Dataset(s)}
We evaluate each of these methods on our set of 162 validation images. This set contains data with clouds, illumination differences, ice mélange, and Landsat-7 SCL errors. This set contains data from all 66 basins in our study, including data from Helheim, which was specifically excluded from CALFIN's training set for validation purposes - as done by Mohajerani et al. This set ensures CALFIN produces consistent results on new data, and addresses concerns raised by Zhang et al. section 7.3 regarding issues of applying this method to new data. See Supplement for detailed metrics on each image in the validation set.

To allow for model inter-comparisons, we also output CALFIN's performance on previous studies' validation sets, where appropriate. These include the 10 Landsat Helheim subsets used in Mohajerani et al. \citep{mohajerani2019} and the 6 TerraSAR-X Jakobshavn subsets used in Zhang et al. \citep{zhang2019}.

\subsection{Classification Accuracy}
The first method calculates the Intersection over Union, or Jaccard index. This is one of the metrics used by Baumhoer et al. \citep{baumhoer2019}. This metric evaluates the degree of overlap between the predicted and ground truth pixel masks of the calving front. It is calculated by dividing the number of pixels in the intersection of two masks over the number of pixels in the union of the two masks. When calculating the IoU between 3 pixel wide edges, this measure is very conservative: 1 pixel of difference results in a score of 0.5000, and scores in that range and above are indicative of human levels of accuracy. When calculating the IoU between large land/ice-ocean-mélange masks, this measure is less conservative, and scores in the range of 0.9000 and above are indicative of human levels of accuracy.


\subsection{Error Estimataion}
The second method is the Mean Distance from the True Front. This is the same metric utilized by Mohajerani et al. \citep{mohajerani2019}, Zhang et al. \citep{zhang2019}, and Baumhoer et al. \citep{baumhoer2019}. This metric evaluates the average distance between closest points between predicted and ground truth fronts. Conceptually, this method resembles numerical integration of the area between two curves, normalized by the average length of the curves. This method can also be seen as a generalization of the method of transects along arbitrarily oriented fronts \citep{mohajerani2019}, \citep{baumhoer2019}. This metric is implemented by averaging the distance between closest pixels in the predicted and ground truth fronts.


\subsection{Validation Results}
The table below prints the above metrics for the appropiate validation sets, as well as the values from comparable studies. 

CALFIN performs well on the Mohajerani validation set, though the reverse is not true. We assert that this shows an improvement in the generalization capability of the CALFIN network over Mohajerani et al. 

CALFIN does not perform competitively on the Zhang validation set, which consists of SAR data. We posit that increasing CALFIN-NN's input resolution, higher resolution SAR data, and additional training will enable CALFIN to incorporate SAR data without further modification. 

Testing on the Antarctic SAR Baumhoer validation set was deemed out of scope for this study on Greenlandic Landsat data.

\begin{table}[h]
\caption{Error estimate and classification accuracy for all available combinations of model-validation sets. }
\centering
\refstepcounter{table}
\label{Error}
\resizebox{0.85\linewidth}{!}{%
\begin{tabular}{lllll}
model-validation & \multicolumn{2}{l}{Distance} & \multicolumn{2}{l}{IoU} \\
 & ~Mean & Median & Ice/Ocean & 3-pixel Front \\
CALFIN-162 & 80.10m,~2.12px & 44.59m, 1.21px &  &  \\
CALFIN-Helheim & 123.01m, 2.32px & 61.47m, 1.22px &  &  \\
CALFIN-Mohajerani & 78.04m, 2.04px &  &  &  \\
CALFIN-Zhang & 336.48m, 4.35px & 190.88m, 2.74px &  & 0.3300 \\
CALFIN-Zhang-ESA-CCI &  &  &  &  \\
Mohajerani-CALFIN-Helheim & 782.85m, 25.24px & 322.47m, 10.47px & 0.9729 & 0.0302 \\
Mohajerani-Mohajerani & 96.31m, 2.71px & - & - & - \\
Zhang-Zhang & 33m, 5.5px & - & - & - \\
Zhang-Zhang-ESA-CCI & 104m, 17.3px & - & - & - \\
Baumhoer-Baumhoer & 108m, 2.69px & - & 0.905 & -
\end{tabular}
}
\end{table}

\subsection{Detection Accuracy}
We show CALFIN's ability to automatically filter images where even humans would not be able to detect calving fronts. To do this, we include 13 images in the test set which contain adverse image conditions that do not contain discernible calving fronts. We calculate the true positive, true negative, false positive, and false negative rates for the entire 162 image data set, and output the confusion matrix below. We note that CALFIN does not output any false positives. This behavior is desired, as it ensures we output fronts conservatively, rather than output incorrect fronts.


\begin{table}[h]
\centering
\refstepcounter{table}
\label{confusion-matrix}
\begin{tabular}{llll}
                               &   & \multicolumn{2}{l}{Predicted}                                                     \\
                               &   & T                                       & F                                       \\
\multirow{2}{*}{Ground Truth~} & T & TPR = 141/149 = 94.63\textbackslash{}\% & FNR = 8/139 = 2.88\textbackslash{}\%    \\
                               & F & FPR = 0/13 = 5.37\textbackslash{}\%     & TNR = 13/13 = 100.00\textbackslash{}\% 
\end{tabular}
\end{table}


%TODO: CLEANUP ALL SECTIONS BELOW!!

\section{Results}
We generate results that are two-fold in nature. First, we release the calving front data we produced throughout the study. Secondly, we release an implementation of CALFIN-NN to the scientific community for use, study, and further development.

\subsection{Results - CALFIN Dataset}
We release the CALFIN dataset, which spans 66 domains across East/West Greenland over the period 1972-2019. This consists of 1703 manually delineated and over 20000 automatically delineated calving fronts.

We provide 2 levels of CALFIN data products. 

Level 0 products include the raw inputs and basic outputs used for CALFIN-NN. These products consist of the raw single-band GeoTiff subsets for each domain, the domain Shapefiles used for subsetting, raw manual/auto-generated coastline pixel masks, and a Quality Assurance image for validation purposes. Note that we do not provide the original Landsat GeoTIFF files. Use cases of Level 0 products may include studies of reproducibility, validation, or training new neural networks.

Level 1 product includes the calving front Shapefile. This product consists only of the isolated, refined, and geo-referenced calving fronts for each time step/domain. Shapefiles and their associated source GeoTIFFs lie in the EPSG:32624 and EPSG:32621 projections for East/West Greenland, respectively.

\subsection{Results - CALFIN-NN Implementation}
We release an implementation of CALFIN-NN, including the weights and architecture we develop throughout this study. It is our intention that any innovations as described in section 3.1 can be applied to other networks and applications. The implementation is written in Python 3, and open access to the code is available at on Github. For access to the network weights/parameters, please see the associated link on Google Drive. For additional details regarding the training regimen, see the following Discussion section 7.1.


\section{Discussion}
We describe information, materials, and methods that are auxiliary to the data and methodology described in this study. This includes the following topics:
\begin{itemize}
    \item We document how our neural network was trained, to encourage transparency and reproduciblility.
    \item We talk about the existing work in this field, specifically surrounding deep learning applied to calving front/coastline extraction.
    \item We perform a model inter-comparison with Mohajerani et al., to study the robustness of both models and their evaluation metrics.
\end{itemize}

\subsection{Discussion - CALFIN-NN Training Data, Training Regimen, and Data Augmentations}

\subsubsection{Training Data}
In order to train our network, we create training data, which consists of preprocessed inputs and ground truth images. This training data consists of 1703 images pairs, with 1541 pairs for training and 162 pairs for validation. This data was made manually in order to ensure consistency and quality in the delineations. The training data has the secondary purpose of providing trustworthy delineations for the CALFIN database, and the calving front masks are output as part of the data release. Raw training images are stored in 16-bit unsigned integer (uint16) RGB images, masks are formatted as uint16 greyscale images, and GeoTIFF subsets are converted to uint16 from their native formats.

\subsubsection{Training Regimen}
We trained the network over 80 epochs, with 4000 batches per epoch, and 8 images per batch. We utilized a K40 Nvidia Tesla GPU, with each epoch taking about 126 minutes to complete, and about 1 week to complete all 80 epochs. We utilize the Adam optimizer, which automatically computes optimal learning rates per parameter. We do use a modified Adam optimizer that accumulates the gradient over 2 batches so that our effective batch size is 16. Additionally, it clips the learning rate at 1e-4, to prevent exploding gradients during training/backpropagation.
We utilize Exponential Linear Units (ELU) over ReLU as our activation function, to prevent vanishing gradients. Our loss function is a weighted combination of binary cross entropy (BCE) and two Jaccard/Intersection-over-Union (IoU) calculations. 
%Insert equation here
This loss function heavily penalizes incorrect predictions (a property of BCE/IoU), heavily favors correct predictions (a property of IoU), and is differentiable (a property of the natural logarithm). Additionally, the weighting heavily favors correct coastline/edge mask predictions, which is critical for increasing calving front prediction accuracy.

\subsubsection{Data Augmentation/Test-time Augmentation}
To avoid overfitting our model, we used data augmentation to generate new inputs from our existing training sets. These augmentations include random flips/transpositions, random Gaussian noise, random sharpen filters, random rotations of up to 12°, random crops, and random scaling. Together, these alterations ensure a greater diversity of data, while still ensuring that the network could train on a fully continuous calving front. We determined through empirical testing that excessive image padding, rotation, and partially clipped calving fronts resulted in sub-optimal performance.

We also utilize test-time augmentations to enhance detection accuracy and confidence. We use 9 overlapping 224x224 windows, and reassemble them into the final 256x256 output mask, as described in section 3. Edge artifacts remain visible in the output masks, but are rendered inconsequential during postprocessing.


\subsection{Discussion - Existing Work}
Our work was performed alongside - and builds on - existing work done in the field.

Mohajerani et al.\citep{mohajerani2019} provides a similar case study using the same type of deep neural network architecture to detect calving fronts. His study is performed on the Greenlandic glacial basins Jakobshavn, Helheim, Sverdrup, and Kangerlussuaq. Mohajerani's methodology displays promising steps towards generalizability and has been preeminent in the field.

Zhang et al.\citep{zhang2019} performs an analysis similar to Mohajerani, albeit in a more specific case study using SAR data of Jakobshavn. The applicability of neural networks as a general methodology for calving front analysis is supported by its successful usage here. 

Baumhoer et al.\citep{baumhoer2019} evaluates modified UNet architectures, as applied to SAR data of Antarctica. Her studies incorporate large spatial context in order to capture whole-coastline delineations. The prospect of larger networks on the order of 768x768 pixels offers the promise of higher resolution/accuracy outputs.

Our methodology shares the same basic approach with the above. We utilize an updated neural network architecture - DeeplabV3+ Xception - that improves upon the base UNet design. Furthermore, we use additional post-processing steps to make the detection more accurate.

\subsection{Discussion - Inter-model Comparison}
We perform an inter-comparison between the performance of CALFIN with respect to the existing studies. In particular, we conduct a comprehensive inter-model comparison between CALFIN and the model developed by Mohajerani et al. Together, these comparisons allow for a robust evaluation of different models and error quantification metrics for automatic calving front detection. To perform this task, we process the validation data used by each other's models, and compare the results.
%TODO
%have Yara/Mike insert results of preprocessed Helheim on Yara's network here?

\begin{figure}[t]
\includegraphics[width=18cm]{helheim_plots_intercomp.png}
\caption{Validation test for 1 of the 10 images used in Mohajerani et al. \textbf{b)} is an RGB image containing the raw image (R), a High Dynamic Range toned image (G), and a Shadows/Highlights enhanced image (B). \textbf{c)} is the RGB output of CALFIN-NN, containing the coastline/edge mask (R) and the land-ice/ocean mask (G). \textbf{e)} compares the fronts from our method (R), the ground truth (G), and Mohajerani et al. (B). The Jaccard index calculates the overlap between our method and the ground truth as a percentage from 0 to 1. \textbf{f)} is a histogram that shows the distribution of distances between points in CALFIN's predicted front and the the ground truth.}
\centering
\end{figure}

Across the all 10 test images in Mohajerani et al., we attain a 102.52 meter (2.10 pixel) mean distance between the predicted and the ground truth fronts. This metric is calculated by finding the closest points from every other point between the predicted and the ground truth front pixel masks. We see that this approaches the level of accuracy achieved in the original study, which was 96.31 meters (1.97 pixels). While our method does not exceed this level of accuracy, it was still able to approach it with no further training done. We conjecture that the relatively higher resolution and the lack of wider context in the input image results in this performance gap.
%To validate this conjecture, we reevaluate our Helheim validation images at higher resolution subsets, but with the added context of the nearby terrain and surroundings. %TODO?



Furthermore, note that \citep{mohajerani2019} obtained the mean error estimate by breaking each delineated front to 1000 smaller segments within x`a small buffer from the fjord walls and calculating the mean deviation between the segments of the true and delineated fronts. Here we calculate the errors by getting the mean distance between each pixel of the delineated front and the closest pixel of the true front. This does not require the fronts to have the same number of pixels, as several pixels can be mapped onto one pixel. It is important to emphasize that the error results can be very sensitive to the error evaluation methodology. By comparing pixels directly, we avoid comparing line segments far from each other due to different lengths of the delineated and true fronts. In that sense, the line-segment methodology of \citep{mohajerani2019} provides a more conservative estimate as it relies on line segments being close to each other.

\textcolor{red}{talk about other differences: landsat stripes not included in training of M19 (Mohajerani et al 2019), M19 used different spectrum band for training, fewer training instances in M19, maybe talk about post-processing difference (least-cost path through front-segmented output, etc.}

Overall, this inter-comparison supports our hypothesis that our methodology is generalizing well and is sufficiently accurate.

\conclusions[Conclusion \& Future Work]
The calving front data products are being released on rolling basis at (datadryad link). %datadryad or NSIDC?
The neural network model is being released on https://github.com/daniel-cheng/CALFIN.

Overall, we accomplished our goal of implementing a method for the automatic calving fronts delineation. Our method utilizes the cutting-edge in deep learning architectures. Our method achieves robustness to minor cloud cover, Landsat 7 scan line errors, and illumination changes. We address issues with generalization through use of additional training data and an improved neural network architecture. We are competitive among existing work being done in this field.  

Based on our error analysis and model inter-comparison, future work may include several directions. We emphasize the need for a standardized training and validation sets. We encourage leveraging more computational resources to enable higher accuracy models. We encourage the production and support of near-real time/multi-satellite data assimilation from multiple models to produce more accurate data.

Our data augmentation, network architecture, and post-processing methods all contribute to the success of our methodology. Ultimately, this work showcases the state-of-the-art in field of deep neural networks as applied to automated calving front detection, and provides a new database of calving fronts for the scientific community.


\codedataavailability{
    The code used to automate the CALFIN pipeline is freely available here: https://github.com/daniel-cheng/CALFIN
    
   The data generated by CALFIN, including Level 0 and 1 data products, are freely available here: (datadryad link)
}

\supplement{.pdf}

\authorcontribution{
    D. Cheng developed the code/model, created the training data, carried out the data processing, and wrote majority of the manuscript.
    Y. Mohajerani performed the model inter-comparison and assisted with the writing of the manuscript.
    MW assisted with the model inter-comparison.
    E. Larour provided direction for the overall study.
    I. Velicogna assisted in organizing collaborators and model inter-comparison.
    M. Wood performed the data preprocessing for the model inter-comparison.
    W. Hayes provided direction on the processing methodology, post-processing algorithms, and error analysis.
    E. Larour, I. Velicogna, and W. Hayes reviewed the manuscript and evaluated the results.
}

\competinginterests{
    The authors declare no competing interests.
}

\begin{acknowledgements}
    This work was conducted as a collaboration between NASA’s Jet Propulsion Laboratory and the University of California, Irvine. 
\end{acknowledgements}

\bibliographystyle{copernicus}
\bibliography{references.bib}  %%% Use the external .bib file (using bibtex).

\end{document}